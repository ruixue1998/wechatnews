import sys
from bs4 import BeautifulSoup, CData
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
from datetime import datetime, timezone, timedelta

def create_rss_en_only(html_filepath, output_filepath):
    """
    è§£æçˆ±èŒƒå„¿æ—©æŠ¥çš„HTMLæ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä¸ªRSSæ–‡ä»¶ã€‚
    RSSæ¡ç›®æ ‡é¢˜ä¸ºä¸­æ–‡ï¼Œæ­£æ–‡å†…å®¹åªä¿ç•™è‹±æ–‡ã€‚

    Args:
        html_filepath (str): è¾“å…¥çš„HTMLæ–‡ä»¶è·¯å¾„ã€‚
        output_filepath (str): è¾“å‡ºçš„RSS XMLæ–‡ä»¶è·¯å¾„ã€‚
    """
    try:
        with open(html_filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{html_filepath}'")
        sys.exit(1)

    soup = BeautifulSoup(html_content, 'lxml')

    # --- 1. åˆ›å»ºRSSåŸºç¡€ç»“æ„ ---
    rss = Element('rss', version='2.0', attrib={'xmlns:content': 'http://purl.org/rss/1.0/modules/content/'})
    channel = SubElement(rss, 'channel')

    # --- 2. æå–å¹¶å¡«å…… Channel å…¨å±€ä¿¡æ¯ ---
    channel_title = soup.find('title').get_text(strip=True) if soup.find('title') else "çˆ±èŒƒå„¿æ—©æŠ¥"
    channel_link = soup.find('link', rel='canonical')['href'] if soup.find('link', rel='canonical') else "https://www.ifanr.com"
    channel_description = soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else "æ¯æ—¥ç§‘æŠ€æ—©æŠ¥"
    
    # è§£æç›¸å¯¹æ—¶é—´ "æ˜¨å¤© HH:MM" ä¸º RFC 822 æ ¼å¼çš„ç»å¯¹æ—¥æœŸ
    pub_date_str = ""
    time_tag = soup.select_one('.article-info__category time')
    if time_tag and "æ˜¨å¤©" in time_tag.get_text(strip=True):
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        time_parts = time_tag.get_text(strip=True).split()
        if len(time_parts) == 2:
            try:
                hour, minute = map(int, time_parts[1].split(':'))
                pub_date_obj = yesterday.replace(hour=hour, minute=minute, second=0, microsecond=0)
                pub_date_str = pub_date_obj.strftime('%a, %d %b %Y %H:%M:%S %z')
            except ValueError:
                pass # æ—¶é—´æ ¼å¼ä¸å¯¹ï¼Œåˆ™å¿½ç•¥
    
    if not pub_date_str: # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºå¤‡ç”¨
        pub_date_str = datetime.now(timezone.utc).strftime('%a, %d %b %Y %H:%M:%S %z')

    SubElement(channel, 'title').text = channel_title
    SubElement(channel, 'link').text = channel_link
    SubElement(channel, 'description').text = channel_description
    SubElement(channel, 'language').text = 'zh-CN'
    SubElement(channel, 'lastBuildDate').text = pub_date_str

    # --- 3. æŸ¥æ‰¾æ‰€æœ‰æ–°é—»æ¡ç›®å¹¶å¤„ç† ---
    content_div = soup.find('div', id='entry-content')
    if not content_div:
        print("é”™è¯¯ï¼šåœ¨HTMLä¸­æ‰¾ä¸åˆ° 'entry-content' å®¹å™¨ã€‚")
        return

    headings = content_div.find_all('h3')

    for h3 in headings:
        item_title = h3.get_text(strip=True)
        
        # è¿‡æ»¤æ‰éæ–°é—»æ ‡é¢˜
        if not item_title or "å‘¨æœ«ä¹Ÿå€¼å¾—ä¸€çœ‹çš„æ–°é—»" in item_title or "æ˜¯å‘¨æœ«å•Š" in item_title:
            continue
            
        item = SubElement(channel, 'item')
        # æ ‡é¢˜ä½¿ç”¨åŸæ–‡ä¸­æ–‡æ ‡é¢˜
        SubElement(item, 'title').text = item_title
        SubElement(item, 'link').text = channel_link
        SubElement(item, 'pubDate').text = pub_date_str
        SubElement(item, 'guid', isPermaLink="false").text = f"{channel_link}#{item_title.replace(' ', '-')}"

        # æå–ä»å½“å‰ h3 åˆ°ä¸‹ä¸€ä¸ª h3 ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ä½œä¸ºæ­£æ–‡
        content_html = []
        for sibling in h3.find_next_siblings():
            if sibling.name == 'h3':
                break  # åˆ°è¾¾ä¸‹ä¸€ä¸ªæ¡ç›®ï¼Œåœæ­¢

            # æ ¸å¿ƒä¿®æ”¹ï¼šæŸ¥æ‰¾åŒè¯­æ ‡ç­¾ï¼Œå¹¶åªä¿ç•™è‹±æ–‡å†…å®¹
            if hasattr(sibling, 'find_all'):
                lang_tags = sibling.find_all(attrs={'ondblclick': True})
                for tag in lang_tags:
                    en_span = tag.find('span', class_='lang-en')
                    if en_span:
                        en_text = en_span.get_text(" ", strip=True)
                        # åˆ›å»ºä¸€ä¸ªæ–°çš„pæ ‡ç­¾ï¼ŒåªåŒ…å«è‹±æ–‡æ–‡æœ¬
                        new_p = soup.new_tag('p')
                        if tag.has_attr('style'):
                            new_p['style'] = tag['style'] # ä¿ç•™åŸæœ‰æ ·å¼
                        new_p.string = en_text
                        # ç”¨æ–°çš„è‹±æ–‡pæ ‡ç­¾æ›¿æ¢æ‰åŸæ¥çš„åŒè¯­æ ‡ç­¾
                        tag.replace_with(new_p)

            content_html.append(str(sibling))
        
        description_text = "".join(content_html)
        description = SubElement(item, 'description')
        # ä½¿ç”¨ CDATA æ¥åŒ…è£¹ HTML å†…å®¹
        description.text = CData(description_text)

    # --- 4. æ ¼å¼åŒ–å¹¶å†™å…¥æ–‡ä»¶ ---
    xml_str = tostring(rss, 'utf-8')
    pretty_xml_str = minidom.parseString(xml_str).toprettyxml(indent="  ")

    with open(output_filepath, 'w', encoding='utf-8') as f:
        f.write(pretty_xml_str)
        
    print(f"ğŸ‰ æˆåŠŸç”Ÿæˆ RSS æ–‡ä»¶ (ä»…è‹±æ–‡æ­£æ–‡): '{output_filepath}'")

# --- è„šæœ¬æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    # å®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶å
    # å°†ä½ çš„HTMLæ–‡ä»¶å‘½åä¸º "ifanr_zaobao.html"
    input_html_file = "ifanr_zaobao.html" 
    output_rss_file = "ifanr_zaobao_rss.xml"
    
    # æ‰§è¡Œè½¬æ¢
    create_rss_en_only(input_html_file, output_rss_file)
