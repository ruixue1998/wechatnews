import sys
import os
import requests # æ–°å¢ï¼šç”¨äºè°ƒç”¨ AI API
from bs4 import BeautifulSoup, CData
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
from datetime import datetime, timezone, timedelta

# --- ã€æ–°å¢ã€‘AI ç¿»è¯‘å‡½æ•°ï¼Œä¸“é—¨ç”¨äºç¿»è¯‘æ ‡é¢˜ ---
def call_ai_for_title_translation(chinese_title):
    """
    è°ƒç”¨ AI API å°†ä¸­æ–‡æ ‡é¢˜ï¼ˆçº¯æ–‡æœ¬ï¼‰ç¿»è¯‘æˆè‹±æ–‡ã€‚
    """
    API_URL = "https://genai-api.thisisray.workers.dev/api/v1/completion"
    # ä»ç¯å¢ƒå˜é‡å®‰å…¨åœ°è¯»å–å¯†é’¥
    AUTH_TOKEN = os.getenv('AI_AUTH_TOKEN')
    if not AUTH_TOKEN:
        print("é”™è¯¯: ç¯å¢ƒå˜é‡ AI_AUTH_TOKEN æœªè®¾ç½®ï¼")
        return None # ç¿»è¯‘å¤±è´¥ï¼Œè¿”å› None

    print(f"æ­£åœ¨ä¸ºæ ‡é¢˜è°ƒç”¨ AI ç¿»è¯‘: '{chinese_title}'")
    
    # ä¸€ä¸ªä¸“é—¨ä¸ºç¿»è¯‘æ ‡é¢˜ä¼˜åŒ–çš„æç¤º
    system_prompt = """You are an expert translator. Translate the following Chinese headline into English.
    Return ONLY the translated English text, without any introductory phrases, explanations, or quotation marks.
    """
    
    payload = { "input": chinese_title, "system": system_prompt, "temperature": 0.3, "model": "gemini-2.5-flash" }
    headers = { "Content-Type": "application/json", "Authorization": f"Bearer {AUTH_TOKEN}" }
    
    try:
        # ä¸º API è°ƒç”¨è®¾ç½®ä¸€ä¸ªåˆç†çš„è¶…æ—¶æ—¶é—´ï¼Œä¾‹å¦‚ 60 ç§’
        response = requests.post(API_URL, json=payload, headers=headers, timeout=60)
        response.raise_for_status() # å¦‚æœè¯·æ±‚å¤±è´¥ (ä¾‹å¦‚ 4xx æˆ– 5xx é”™è¯¯), åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸
        translated_text = response.text.strip()
        print(f"  -> ç¿»è¯‘æˆåŠŸ: '{translated_text}'")
        return translated_text
    except requests.exceptions.Timeout:
        print(f"  -> é”™è¯¯: AI API è¯·æ±‚è¶…æ—¶ã€‚")
        return None
    except requests.exceptions.RequestException as e:
        print(f"  -> é”™è¯¯: AI API è¯·æ±‚å¤±è´¥ã€‚è¯¦æƒ…: {e}")
        return None

# --- ä¸»å‡½æ•°ï¼ˆå·²ä¿®æ”¹ï¼‰ ---
def create_rss_en_only(html_filepath, output_filepath):
    """
    è§£æçˆ±èŒƒå„¿æ—©æŠ¥çš„HTMLæ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä¸ªRSSæ–‡ä»¶ã€‚
    ã€å·²ä¿®æ”¹ã€‘RSSæ¡ç›®æ ‡é¢˜ä¼šå…ˆé€šè¿‡AIç¿»è¯‘æˆè‹±æ–‡ï¼Œæ­£æ–‡å†…å®¹åªä¿ç•™è‹±æ–‡ã€‚
    """
    try:
        with open(html_filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{html_filepath}'")
        sys.exit(1)

    soup = BeautifulSoup(html_content, 'lxml')

    # --- 1. åˆ›å»ºRSSåŸºç¡€ç»“æ„ ---
    rss = Element('rss', version='2.0', attrib={'xmlns:content': 'http://purl.org/rss/1.0/modules/content/'})
    channel = SubElement(rss, 'channel')

    # --- 2. Channel å…¨å±€ä¿¡æ¯ ---
    SubElement(channel, 'title').text = "Daily News" 
    
    pub_date_str = datetime.now(timezone.utc).strftime('%a, %d %b %Y %H:%M:%S %z')
    # å°è¯•ä»HTMLä¸­è§£ææ›´ç²¾ç¡®çš„æ—¥æœŸ
    time_tag = soup.select_one('.article-info__category time')
    if time_tag:
        time_text = time_tag.get_text(strip=True)
        try:
            # å‡è®¾æ ¼å¼æ˜¯ "æ˜¨å¤© HH:MM"
            if "æ˜¨å¤©" in time_text:
                yesterday = datetime.now(timezone.utc) - timedelta(days=1)
                time_parts = time_text.split()[-1].split(':')
                hour, minute = int(time_parts[0]), int(time_parts[1])
                pub_date_obj = yesterday.replace(hour=hour, minute=minute, second=0, microsecond=0)
                pub_date_str = pub_date_obj.strftime('%a, %d %b %Y %H:%M:%S %z')
        except (ValueError, IndexError):
            pass # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„å½“å‰æ—¶é—´

    SubElement(channel, 'lastBuildDate').text = pub_date_str

    # --- 3. æŸ¥æ‰¾æ‰€æœ‰æ–°é—»æ¡ç›®å¹¶å¤„ç† ---
    content_div = soup.find('div', class_='entry-content') # ä½¿ç”¨ class é€‰æ‹©å™¨å¯èƒ½æ›´é€šç”¨
    if not content_div:
        print("é”™è¯¯ï¼šåœ¨HTMLä¸­æ‰¾ä¸åˆ° class='entry-content' çš„å®¹å™¨ã€‚")
        return

    headings = content_div.find_all('h3')

    print("\n--- å¼€å§‹å¤„ç†æ–°é—»æ¡ç›®å¹¶ç¿»è¯‘æ ‡é¢˜ ---")
    for h3 in headings:
        original_title_cn = h3.get_text(strip=True)
        
        if not original_title_cn or "å‘¨æœ«ä¹Ÿå€¼å¾—ä¸€çœ‹çš„æ–°é—»" in original_title_cn or "æ˜¯å‘¨æœ«å•Š" in original_title_cn:
            continue

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘è°ƒç”¨ AI ç¿»è¯‘æ ‡é¢˜
        item_title = call_ai_for_title_translation(original_title_cn)
        
        # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œåˆ™å›é€€åˆ°åŸå§‹ä¸­æ–‡æ ‡é¢˜å¹¶æ‰“å°è­¦å‘Š
        if not item_title:
            print(f"  -> è­¦å‘Š: æ ‡é¢˜ç¿»è¯‘å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹ä¸­æ–‡æ ‡é¢˜: '{original_title_cn}'")
            item_title = original_title_cn
            
        item = SubElement(channel, 'item')
        SubElement(item, 'title').text = item_title
        SubElement(item, 'pubDate').text = pub_date_str
        SubElement(item, 'guid', isPermaLink="false").text = item_title.replace(' ', '-')

        # æå–ä»å½“å‰ h3 åˆ°ä¸‹ä¸€ä¸ª h3 ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ä½œä¸ºæ­£æ–‡
        content_html = []
        for sibling in h3.find_next_siblings():
            if sibling.name == 'h3':
                break

            # è¿™éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜ï¼Œç”¨äºæå–åŒè¯­HTMLä¸­çš„è‹±æ–‡éƒ¨åˆ†
            if hasattr(sibling, 'find_all'):
                # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬è¿›è¡Œæ“ä½œï¼Œé¿å…ä¿®æ”¹åŸå§‹çš„ soup å¯¹è±¡
                temp_sibling = BeautifulSoup(str(sibling), 'lxml').body.next
                
                lang_tags = temp_sibling.find_all(attrs={'ondblclick': True})
                for tag in lang_tags:
                    en_span = tag.find('span', class_='lang-en')
                    if en_span:
                        en_text = en_span.get_text(" ", strip=True)
                        new_p = soup.new_tag('p')
                        if tag.has_attr('style'):
                            new_p['style'] = tag['style']
                        new_p.string = en_text
                        tag.replace_with(new_p)
                
                content_html.append(str(temp_sibling))
            else:
                 content_html.append(str(sibling))
        
        description_text = "".join(content_html)
        description = SubElement(item, 'description')
        description.text = CData(description_text)

    # --- 4. æ ¼å¼åŒ–å¹¶å†™å…¥æ–‡ä»¶ ---
    xml_str = tostring(rss, 'utf-8')
    pretty_xml_str = minidom.parseString(xml_str).toprettyxml(indent="  ")

    with open(output_filepath, 'w', encoding='utf-8') as f:
        f.write(pretty_xml_str)
        
    print(f"\nğŸ‰ æˆåŠŸç”Ÿæˆ RSS æ–‡ä»¶ (æ ‡é¢˜å·²ç¿»è¯‘ï¼Œæ­£æ–‡ä»…è‹±æ–‡): '{output_filepath}'")

# --- è„šæœ¬æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    input_html_file = "DailyNews.html" 
    output_rss_file = "DailyNews.xml"
    
    # ç¡®ä¿ AI è®¤è¯ä»¤ç‰Œå­˜åœ¨
    if not os.getenv('AI_AUTH_TOKEN'):
        print("è‡´å‘½é”™è¯¯: ç¯å¢ƒå˜é‡ AI_AUTH_TOKEN æœªè®¾ç½®ã€‚è¯·å…ˆè®¾ç½®æ­¤å˜é‡å†è¿è¡Œè„šæœ¬ã€‚")
        sys.exit(1)

    create_rss_en_only(input_html_file, output_rss_file)
