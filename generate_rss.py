import sys
import os
import requests
from bs4 import BeautifulSoup, CData
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
from datetime import datetime, timezone, timedelta

# --- æ–°å¢: AI ç¿»è¯‘å‡½æ•° (ä¸ºçº¯æ–‡æœ¬æ ‡é¢˜ä¼˜åŒ–) ---
def translate_titles_with_ai(titles_list):
    """
    è°ƒç”¨ AI APIï¼Œå°†ä¸€ä¸ªä¸­æ–‡æ ‡é¢˜åˆ—è¡¨æ‰¹é‡ç¿»è¯‘æˆè‹±æ–‡æ ‡é¢˜åˆ—è¡¨ã€‚
    """
    # API å’Œè®¤è¯ä¿¡æ¯ä¸æ‚¨æä¾›çš„ç¤ºä¾‹ä¿æŒä¸€è‡´
    API_URL = "https://genai-api.thisisray.workers.dev/api/v1/completion"
    AUTH_TOKEN = os.getenv('AI_AUTH_TOKEN')
    
    if not AUTH_TOKEN:
        print("é”™è¯¯: ç¯å¢ƒå˜é‡ AI_AUTH_TOKEN æœªè®¾ç½®ï¼")
        sys.exit(1)

    # ä½¿ç”¨ç‰¹æ®Šåˆ†éš”ç¬¦è¿æ¥æ‰€æœ‰æ ‡é¢˜ï¼Œä»¥ä¾¿ AI ä¸€æ¬¡æ€§å¤„ç†
    # è¿™æ ·æ¯”ä¸ºæ¯ä¸ªæ ‡é¢˜å•ç‹¬è¯·æ±‚ä¸€æ¬¡ API æ›´é«˜æ•ˆ
    separator = "|||"
    combined_titles = separator.join(titles_list)

    print(f"æ­£åœ¨å‡†å¤‡è°ƒç”¨ AI API ç¿»è¯‘ {len(titles_list)} ä¸ªæ ‡é¢˜...")
    
    system_prompt = f"""You are an expert translator. You will receive a list of news headlines in Chinese, joined by a specific separator ('{separator}').
Your task is to:
1. Translate each headline accurately into English.
2. Return the translated headlines, also joined by the exact same separator ('{separator}').
3. Do NOT add any numbering, explanations, or any text other than the translated headlines and the separator.
4. The number of headlines you return must exactly match the number of headlines you receive.

Example Input:
è‹¹æœå‘å¸ƒæ–°æ¬¾ MacBook Pro|||ç‰¹æ–¯æ‹‰å®£å¸ƒä¸‹è°ƒ Model 3 ä»·æ ¼

Example Output:
Apple Releases New MacBook Pro|||Tesla Announces Price Cut for Model 3
"""
    
    payload = {
        "input": combined_titles,
        "system": system_prompt,
        "temperature": 0.5,
        "model": "gemini-pro" # ä½¿ç”¨ä¸€ä¸ªèƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹ä»¥ä¿è¯ç¿»è¯‘è´¨é‡
    }
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {AUTH_TOKEN}"
    }
    
    try:
        print("æ­£åœ¨å‘ AI API å‘é€æ ‡é¢˜ç¿»è¯‘è¯·æ±‚...")
        response = requests.post(API_URL, json=payload, headers=headers, timeout=300)
        response.raise_for_status()
        
        ai_response_text = response.text.strip()
        translated_titles = ai_response_text.split(separator)
        
        # æ ¡éªŒè¿”å›çš„æ ‡é¢˜æ•°é‡æ˜¯å¦ä¸å‘é€çš„ä¸€è‡´
        if len(translated_titles) != len(titles_list):
            print("é”™è¯¯: AI è¿”å›çš„ç¿»è¯‘æ ‡é¢˜æ•°é‡ä¸åŸæ–‡ä¸åŒ¹é…ï¼")
            print(f"åŸæ–‡æ•°é‡: {len(titles_list)}, ç¿»è¯‘åæ•°é‡: {len(translated_titles)}")
            print("AI è¿”å›å†…å®¹:", ai_response_text)
            return None

        print("âœ… AI æˆåŠŸè¿”å›äº†ç¿»è¯‘åçš„æ ‡é¢˜ã€‚")
        return translated_titles
        
    except requests.exceptions.Timeout:
        print("é”™è¯¯: AI API è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡300ç§’ï¼‰ã€‚")
        return None
    except requests.exceptions.RequestException as e:
        print(f"é”™è¯¯: AI API è¯·æ±‚å¤±è´¥ã€‚è¯¦æƒ…: {e}")
        if hasattr(e, 'response') and e.response is not None:
             print("æœåŠ¡å™¨å“åº”:", e.response.text)
        return None

def create_rss_en_only(html_filepath, output_filepath):
    """
    è§£æçˆ±èŒƒå„¿æ—©æŠ¥çš„HTMLæ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä¸ªRSSæ–‡ä»¶ã€‚
    RSSæ¡ç›®æ ‡é¢˜ç”± AI ç¿»è¯‘æˆè‹±æ–‡ï¼Œæ­£æ–‡å†…å®¹åªä¿ç•™è‹±æ–‡ã€‚
    """
    try:
        with open(html_filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{html_filepath}'")
        sys.exit(1)

    soup = BeautifulSoup(html_content, 'lxml')
    content_div = soup.find('div', id='entry-content')
    if not content_div:
        print("é”™è¯¯ï¼šåœ¨HTMLä¸­æ‰¾ä¸åˆ° 'entry-content' å®¹å™¨ã€‚")
        return

    # --- 1. æå–æ‰€æœ‰ä¸­æ–‡æ ‡é¢˜ ---
    headings = content_div.find_all('h3')
    chinese_titles = []
    valid_headings = []
    for h3 in headings:
        title_text = h3.get_text(strip=True)
        # è¿‡æ»¤æ‰æ— æ•ˆæˆ–æ¨å¹¿æ€§è´¨çš„æ ‡é¢˜
        if title_text and "å‘¨æœ«ä¹Ÿå€¼å¾—ä¸€çœ‹çš„æ–°é—»" not in title_text and "æ˜¯å‘¨æœ«å•Š" not in title_text:
            chinese_titles.append(title_text)
            valid_headings.append(h3)

    if not chinese_titles:
        print("è­¦å‘Šï¼šåœ¨ HTML ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–°é—»æ ‡é¢˜ã€‚")
        return

    # --- 2. è°ƒç”¨ AI ç¿»è¯‘æ‰€æœ‰æ ‡é¢˜ ---
    english_titles = translate_titles_with_ai(chinese_titles)
    if not english_titles:
        print("ç”±äº AI ç¿»è¯‘å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç”Ÿæˆ RSS æ–‡ä»¶ã€‚")
        return
        
    # åˆ›å»ºä¸€ä¸ªä»ä¸­æ–‡æ ‡é¢˜åˆ°è‹±æ–‡æ ‡é¢˜çš„æ˜ å°„ï¼Œæ–¹ä¾¿åç»­æŸ¥æ‰¾
    title_translation_map = dict(zip(chinese_titles, english_titles))

    # --- 3. åˆ›å»ºRSSåŸºç¡€ç»“æ„ ---
    rss = Element('rss', version='2.0', attrib={'xmlns:content': 'http://purl.org/rss/1.0/modules/content/'})
    channel = SubElement(rss, 'channel')
    SubElement(channel, 'title').text = "Daily News (AI Translated)"
    
    # è§£æå‘å¸ƒæ—¥æœŸ
    pub_date_str = ""
    time_tag = soup.select_one('.article-info__category time')
    if time_tag and "æ˜¨å¤©" in time_tag.get_text(strip=True):
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        time_parts = time_tag.get_text(strip=True).split()
        if len(time_parts) == 2:
            try:
                hour, minute = map(int, time_parts[1].split(':'))
                pub_date_obj = yesterday.replace(hour=hour, minute=minute, second=0, microsecond=0)
                pub_date_str = pub_date_obj.strftime('%a, %d %b %Y %H:%M:%S %z')
            except ValueError:
                pass
    
    if not pub_date_str:
        pub_date_str = datetime.now(timezone.utc).strftime('%a, %d %b %Y %H:%M:%S %z')

    SubElement(channel, 'lastBuildDate').text = pub_date_str

    # --- 4. éå†æœ‰æ•ˆæ ‡é¢˜å¹¶ç”Ÿæˆ RSS Item ---
    for h3 in valid_headings:
        original_title = h3.get_text(strip=True)
        translated_title = title_translation_map.get(original_title)
        
        if not translated_title:
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°æ ‡é¢˜ '{original_title}' å¯¹åº”çš„ç¿»è¯‘ï¼Œè·³è¿‡æ­¤æ¡ç›®ã€‚")
            continue
            
        item = SubElement(channel, 'item')
        
        # ä½¿ç”¨ç¿»è¯‘åçš„è‹±æ–‡æ ‡é¢˜
        SubElement(item, 'title').text = translated_title
        SubElement(item, 'pubDate').text = pub_date_str
        
        # ä½¿ç”¨è‹±æ–‡æ ‡é¢˜ç”Ÿæˆ GUID
        SubElement(item, 'guid', isPermaLink="false").text = translated_title.replace(' ', '-')

        # æå–ä»å½“å‰ h3 åˆ°ä¸‹ä¸€ä¸ª h3 ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ä½œä¸ºæ­£æ–‡
        content_html = []
        for sibling in h3.find_next_siblings():
            if sibling.name == 'h3':
                break

            # è¿™éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜ï¼Œä¾ç„¶æ˜¯æå–åŒè¯­å†…å®¹ä¸­çš„è‹±æ–‡éƒ¨åˆ†
            if hasattr(sibling, 'find_all'):
                lang_tags = sibling.find_all(attrs={'ondblclick': True})
                for tag in lang_tags:
                    en_span = tag.find('span', class_='lang-en')
                    if en_span:
                        en_text = en_span.get_text(" ", strip=True)
                        new_p = soup.new_tag('p')
                        if tag.has_attr('style'):
                            new_p['style'] = tag['style']
                        new_p.string = en_text
                        tag.replace_with(new_p)

            content_html.append(str(sibling))
        
        description_text = "".join(content_html)
        description = SubElement(item, 'description')
        description.text = CData(description_text)

    # --- 5. æ ¼å¼åŒ–å¹¶å†™å…¥æ–‡ä»¶ ---
    xml_str = tostring(rss, 'utf-8')
    pretty_xml_str = minidom.parseString(xml_str).toprettyxml(indent="  ")

    with open(output_filepath, 'w', encoding='utf-8') as f:
        f.write(pretty_xml_str)
        
    print(f"ğŸ‰ æˆåŠŸç”Ÿæˆ RSS æ–‡ä»¶ (AI ç¿»è¯‘æ ‡é¢˜ + ä»…è‹±æ–‡æ­£æ–‡): '{output_filepath}'")

# --- è„šæœ¬æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    # åœ¨è¿è¡Œå‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»è®¾ç½®äº†ç¯å¢ƒå˜é‡ AI_AUTH_TOKEN
    # ä¾‹å¦‚: export AI_AUTH_TOKEN="your_secret_token"
    if not os.getenv('AI_AUTH_TOKEN'):
         print("é‡è¦æç¤ºï¼šè¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ 'AI_AUTH_TOKEN' å†è¿è¡Œæ­¤è„šæœ¬ã€‚")
    else:
        input_html_file = "DailyNews.html" 
        output_rss_file = "DailyNews_AI_Translated.xml" # ä½¿ç”¨æ–°æ–‡ä»¶åä»¥é¿å…è¦†ç›–æ—§æ–‡ä»¶
        
        create_rss_en_only(input_html_file, output_rss_file)
